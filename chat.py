    def probability_mass(
        self,
        lhs: Nonterminal,
        rhs_options: Sequence[Tuple],
    ) -> Dict[Tuple, float]:
        """
        Compute probabilities for each right-hand side option using token-level LLM probabilities.
        This is called by the parser to evaluate P(rhs|lhs) for rules.
        
        For token-level approach, we use the cached span probabilities computed by get_span_probabilities.
        """
        # We need span probabilities to be pre-computed
        if not hasattr(self, "_span_probs"):
            raise ValueError("Span probabilities must be pre-computed with get_span_probabilities before calling probability_mass")
        
        probs = {}
        for rhs in rhs_options:
            # This will be implemented by the TokenLevelViterbiParser that has context of spans
            # Here we provide a placeholder implementation
            probs[rhs] = 1.0 / len(rhs_options)
            
        return probs
        
    def set_text_and_precompute(self, tokens):
        """
        Set the current text being parsed and precompute span probabilities.
        This should be called before parsing.
        """
        self._text = " ".join(tokens)
        n = len(tokens)
        
        # Generate all possible spans
        spans = [(i, j) for i in range(n) for j in range(i+1, n+1)]
        
        # Get span probabilities from LLM
        self._span_probs = self.get_span_probabilities(self._text, spans)
        
        return self._span_probs
    
    
    
    
import logging
from typing import Dict, Tuple, List, Sequence, Iterable
from nltk.parse.viterbi import ViterbiParser
from nltk.grammar import Nonterminal, ProbabilisticProduction, PCFG
from nltk.tree import ProbabilisticTree, Tree
import math
from functools import reduce
from provider import TokenLevelProbabilityProvider

# Logging
LOGGER = logging.getLogger(__name__)
LOGGER.addHandler(logging.NullHandler())

class TokenLevelViterbiParser(ViterbiParser):
    """
    A ViterbiParser that integrates token-level probabilities from an LLM.
    
    This parser extends NLTK's ViterbiParser to use token-level probabilities
    generated by a local LLM. It interpolates between the grammar's original 
    probabilities and the LLM's probabilities using a parameter theta.
    """
    
    def __init__(
        self,
        grammar: PCFG,
        token_provider: TokenLevelProbabilityProvider,
        *,
        theta: float = 0.8,
        trace: int = 0,
    ):
        """
        Initialize the parser with a grammar and token-level probability provider.
        
        Args:
            grammar: The PCFG grammar to use for parsing
            token_provider: The TokenLevelProbabilityProvider to get LLM probabilities from
            theta: Weight for interpolation (θ*grammar_prob + (1-θ)*llm_prob)
            trace: Trace level for debugging output
        """
        super().__init__(grammar, trace=trace)
        self._token_provider = token_provider
        self._theta = theta
        if not 0.0 <= theta <= 1.0:
            raise ValueError("θ must be in [0, 1]")
            
    def parse(self, tokens):
        """
        Parse the given tokens using the grammar and token-level probabilities.
        
        This method extends the standard Viterbi parser by incorporating
        token-level probabilities from the LLM.
        """
        tokens = list(tokens)
        self._grammar.check_coverage(tokens)
        
        # Precompute span probabilities using the LLM
        LOGGER.info("Precomputing span probabilities using LLM...")
        span_probs = self._token_provider.set_text_and_precompute(tokens)
        
        # Store the tokens for use in probability calculation
        self._current_tokens = tokens
        
        # Initialize the most likely constituent table
        constituents = {}
        
        # Initialize with lexical items
        if self._trace:
            print("Inserting tokens into the most likely constituents table...")
        for index in range(len(tokens)):
            token = tokens[index]
            constituents[index, index + 1, token] = token
            if self._trace > 1:
                self._trace_lexical_insertion(token, index, len(tokens))
        
        # Build the parse table bottom-up
        for length in range(1, len(tokens) + 1):
            if self._trace:
                print(
                    "Finding the most likely constituents"
                    + " spanning %d text elements..." % length
                )
            for start in range(len(tokens) - length + 1):
                span = (start, start + length)
                self._add_constituents_spanning(span, constituents, tokens)
        
        # Return the tree that spans the entire text & has the right category
        tree = constituents.get((0, len(tokens), self._grammar.start()))
        if tree is not None:
            yield tree
    
    def _add_constituents_spanning(self, span, constituents, tokens):
        """
        Find constituents that might cover a span, using interpolated probabilities.
        
        This overrides the standard method to use interpolated probabilities between
        the grammar and the LLM.
        """
        # Similar to the original implementation, but with probability interpolation
        changed = True
        while changed:
            changed = False
            
            # Find all ways to instantiate grammar productions that cover the span
            instantiations = self._find_instantiations(span, constituents)
            
            # Process each production instantiation with interpolated probabilities
            for production, children in instantiations:
                subtrees = [c for c in children if isinstance(c, Tree)]
                
                # Calculate grammar probability
                grammar_prob = reduce(lambda pr, t: pr * t.prob(), subtrees, production.prob())
                
                # Calculate LLM probability for this rule application
                llm_prob = self._get_llm_probability(production, children, span)
                
                # Interpolate probabilities
                interpolated_prob = self._theta * grammar_prob + (1.0 - self._theta) * llm_prob
                
                # Create tree with interpolated probability
                node = production.lhs().symbol()
                tree = ProbabilisticTree(node, children, prob=interpolated_prob)
                
                # Update the constituents table if this is a better parse
                c = constituents.get((span[0], span[1], production.lhs()))
                if self._trace > 1:
                    if c is None or c != tree:
                        if c is None or c.prob() < tree.prob():
                            print("   Insert:", end=" ")
                        else:
                            print("  Discard:", end=" ")
                        self._trace_production(production, interpolated_prob, span, len(tokens))
                        print(f"  (Grammar: {grammar_prob:.4f}, LLM: {llm_prob:.4f})")
                
                if c is None or c.prob() < tree.prob():
                    constituents[span[0], span[1], production.lhs()] = tree
                    changed = True
    
    def _get_llm_probability(self, production, children, span):
        """
        Calculate the LLM probability for a production application.
        
        This method computes the probability of applying a production at a specific
        span using the token-level probabilities from the LLM.
        """
        lhs = production.lhs()
        start, end = span
        
        # Get the probability of the LHS for this span
        lhs_prob = self._token_provider._span_probs.get((start, end), {}).get(lhs, 0.001)
        
        # For terminal productions, just return the LHS probability
        if all(not isinstance(c, Tree) for c in children):
            return lhs_prob
        
        # For non-terminal productions, multiply LHS probability by
        # conditional probabilities of children
        child_probs = 1.0
        for child in children:
            if isinstance(child, Tree):
                child_span = self._get_tree_span(child, span[0])
                child_lhs = Nonterminal(child.label())
                child_prob = self._token_provider._span_probs.get(child_span, {}).get(child_lhs, 0.001)
                child_probs *= child_prob
        
        # Avoid zero probabilities
        if lhs_prob == 0 or child_probs == 0:
            return 0.001
            
        return lhs_prob * child_probs
    
    def _get_tree_span(self, tree, start_offset):
        """
        Calculate the span covered by a subtree given the start offset.
        
        This is a helper method to identify the span of text covered by a subtree.
        """
        # Calculate the span based on the number of leaf nodes
        length = len(tree.leaves())
        return (start_offset, start_offset + length)
    
    import logging
from nltk import PCFG, Nonterminal
from nltk.grammar import Production
from provider import TokenLevelProbabilityProvider
from local_llm import LocalLLM
from token_viterbi import TokenLevelViterbiParser
import torch

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    # Define a small grammar for testing
    grammar_str = """
    S -> NP VP [1.0]
    NP -> Det N [0.6] | Pronoun [0.4]
    VP -> V NP [0.7] | V [0.3]
    Det -> 'the' [0.8] | 'a' [0.2]
    N -> 'dog' [0.4] | 'cat' [0.6]
    V -> 'chased' [0.6] | 'saw' [0.4]
    Pronoun -> 'he' [0.5] | 'she' [0.5]
    """
    grammar = PCFG.fromstring(grammar_str)
    
    # Extract all nonterminals from the grammar
    nonterminals = {str(prod.lhs()) for prod in grammar.productions()}
    
    logger.info("Initializing LLM")
    llm = LocalLLM(model_name="llama3_1-70b")
    
    logger.info(f"Creating token level probability provider with nonterminals: {nonterminals}")
    provider = TokenLevelProbabilityProvider(llm, nonterminals, cache_size=2048)

    logger.info("Creating token-level viterbi parser")
    parser = TokenLevelViterbiParser(
        grammar=grammar,
        token_provider=provider,
        theta=0.8, 
        trace=2,  # Detailed tracing
    )
    
    test_sentences = [
        "the dog chased the cat",
        "she saw a dog",
        "he chased",
    ]
    
    for sentence in test_sentences:
        logger.info(f"\nParsing sentence: '{sentence}'")
        tokens = sentence.split()
        
        parse_results = list(parser.parse(tokens))
        
        if parse_results:
            for i, tree in enumerate(parse_results):
                logger.info(f"Parse {i+1}:")
                logger.info(f"Tree: {tree}")
                logger.info(f"Probability: {tree.prob()}")
        else:
            logger.info("No parse found.")

if __name__ == "__main__":
    main()